{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "robots_train = pd.read_csv('robots_train.csv')\n",
    "robots_test = pd.read_csv('robots_test.csv')\n",
    "evil_train = pd.read_csv('evil_train.csv')\n",
    "evil_test = pd.read_csv('evil_test.csv')\n",
    "                           \n",
    "\n",
    "robots_train = robots_train.drop('Unnamed: 0', axis = 1)\n",
    "robots_test = robots_test.drop('Unnamed: 0', axis = 1)\n",
    "evil_train = evil_train.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get the info and summary stats for the `robots_train` dataframe.\n",
    "    - What kinds of data types do you have?\n",
    "    - Are there null values?\n",
    "    - What's the scale of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make a seaborn pairplot to look at comparisons between the variables by filling in the blanks below. Make sure to display whether a robot is evil or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "train_whole = pd.concat([robots_train, evil_train], axis = 1)\n",
    "\n",
    "sns.pairplot(data= 'TYPE HERE' , hue = 'TYPE HERE' , plot_kws = {'alpha': 0.5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Wait, what's going on with the `age` variable? Looks like a boxplot wouldn't capture that weirdness. Use a violin plot to show distributions of the non-target variables, splitting by evilness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER HERE BY FILLING IN APPROPRIATE CODE\n",
    "\n",
    "# Note, to get the plot in a form seaborn like for this, we have to use pd.melt - this one should be pretty easy\n",
    "train_long = pd.melt(train_whole, id_vars = ['evil'])\n",
    "\n",
    "sns.violinplot(data=train_long[train_long['variable'] == 'TYPE HERE'], x = 'variable', y = 'value', hue = 'evil', split =True ,inner=\"quart\")\n",
    "sns.violinplot(data=train_long[train_long['variable'] == 'TYPE HERE'], x = 'variable', y = 'value', hue = 'evil', split =True ,inner=\"quart\", legend = False)\n",
    "sns.violinplot(data=train_long[train_long['variable'] == 'TYPE HERE'], x = 'variable', y = 'value', hue = 'evil', split =True ,inner=\"quart\", legend = False)\n",
    "sns.violinplot(data=train_long[train_long['variable'] == 'TYPE HERE'], x = 'variable', y = 'value', hue = 'evil', split =True ,inner=\"quart\", legend = False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Ok, between the `pairplot` and `violinplot` above, it looks like there may be outliers. Let's implement a detection scheme to remove them and replace them with Nulls, which we will impute in later steps. Note we'll do this all in one swoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Detection:\n",
    "\n",
    "#Create an empty dataframe to hold the robots_train_data with outliers replaced with nans\n",
    "train_nan_outliers = pd.DataFrame()\n",
    "robots_train_nans  = pd.DataFrame()\n",
    "\n",
    "# Loop over training columns - note: we don't zscore the target\n",
    "for pred in robots_train.columns:\n",
    "    #print(pred)\n",
    "    robots_train_nans[pred] = robots_train[pred].apply(lambda x: x if (x - robots_train[pred].mean())/robots_train[pred].std() <= 3 else None)\n",
    "\n",
    "\n",
    "#Use .value_counts() to examine the number of Nones after detection and replacement\n",
    "print(robots_train_nans['ENTER TEXT HERE'].#INSERT APPROPRIATE METHOD HERE\n",
    "    .value_counts())\n",
    "print(robots_train_nans['ENTER TEXT HERE'].#INSERT APPROPRIATE METHOD HERE\n",
    "    .value_counts())\n",
    "print(robots_train_nans['ENTER TEXT HERE'].#INSERT APPROPRIATE METHOD HERE\n",
    "    .value_counts())\n",
    "print(robots_train_nans['ENTER TEXT HERE'].#INSERT APPROPRIATE METHOD HERE\n",
    "    .value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Just for a sanity check, let's see how the data look now after outlier removal. Recreate your violin plots from above using the new outlier-free data. Is there anything you notice about the distributions now that outliers are gone? Note that the quartiles are inside of the violin plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First put the outlier-free train data back together with the target variables\n",
    "\n",
    "train_and_targ_with_nans = #SOME PANDAS FUNCTION HERE#([robots_train_nans, evil_train], axis = 1)\n",
    "\n",
    "# Do the melting again like above\n",
    "train_long = pd.melt(train_and_targ_with_nans, id_vars = ['evil'])\n",
    "\n",
    "sns.violinplot(data=train_long[train_long['variable'] == 'ENTER TEXT HERE'], x = 'variable', y = 'value', hue = 'evil', split =True ,inner=\"quart\")\n",
    "sns.violinplot(data=train_long[train_long['variable'] == 'ENTER TEXT HERE'], x = 'variable', y = 'value', hue = 'evil', split =True ,inner=\"quart\", legend = False)\n",
    "sns.violinplot(data=train_long[train_long['variable'] == 'ENTER TEXT HERE'], x = 'variable', y = 'value', hue = 'evil', split =True ,inner=\"quart\", legend = False)\n",
    "plt.show()\n",
    "sns.violinplot(data=train_long[train_long['variable'] == 'ENTER TEXT HERE'], x = 'variable', y = 'value', hue = 'evil', split =True ,inner=\"quart\", legend = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling, Imputing, Rebalancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Looking at the plots above, our data seems to be on different scales. We should get all the fields on a comparable scale (Why?). Also we have all these null values from the original data and the outlier removal, so let's impute them. Let's make this a little more interesting and use a KNNImputer. Below, I've sketched out the steps of putting this all together in a `Pipeline` object along with dealing with the imbalance via SMOTE by oversampling the evil class. Fill in the necessaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that for imbalanced data pipelines, we have to use the imblearn package's Pipeline objects\n",
    "# Uncomment the following line and run in to install that package - should only have to do once per new codespace\n",
    "# pip install imblearn\n",
    "\n",
    "\n",
    "# Scale variables - Mostly look normalish - for age, let's just see what happens\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# KNN Impute Outliers and nulls\n",
    "from sklearn.impute import KNNImputer\n",
    "#Import imblearn pipeline and SMOTE classes\n",
    "from imblearn.pipeline import Pipeline as imbPipe\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "preprocess = imbPipe(\n",
    "    [\n",
    "        ('scaler', # PUT SCALING STEP IN HERE WITH ANY OPTIONS YOU THINK APPROPRIATE),\n",
    "        ('knn_imp', #PUT KNN IMPUTER HERE WITH ANY OPTIONS YOU THINK APPROPRIATE)\n",
    "        ('smote', SMOTE(random_state=1234)) # I went ahead and did the SMOTE for you\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# I did this part -note we use fit_resample instead of fit_transform\n",
    "X_train_smote, y_train_smote = preprocess.fit_resample(robots_train_nans, evil_train)\n",
    "\n",
    "# Verify that the data has been rebalanced in the next line:\n",
    "print(#Balance confirmation here)\n",
    "\n",
    "# Lastly, just verify SMOTE worked by doing a new pairplot\n",
    "sns.pairplot(data = pd.concat([pd.DataFrame(X_train_smote, columns = #Some Object Here.get_feature_names_out()), y_train_smote], axis = 1), hue = 'evil')#, alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some stuff that'll be used in all the models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_folds = ? # choose your k\n",
    "kf = StratifiedKFold(n_splits=n_folds, random_state=43, shuffle=True)\n",
    "\n",
    "scoring_dict = {\n",
    "     'accuracy': 'accuracy',\n",
    "     'precision': 'precision',\n",
    "     'recall': 'recall',\n",
    "     'f1': 'f1'\n",
    "}\n",
    "\n",
    "def confusion_matrix_scorer(clf, X, y):\n",
    "     y_pred = clf.predict(X)\n",
    "     cm = confusion_matrix(y, y_pred)\n",
    "     return {'tn': cm[0, 0], 'fp': cm[0, 1],\n",
    "             'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a quick function to display our results\n",
    "\n",
    "def get_eval_metrics_report(reg_cv_results, cm_cv_results):\n",
    "    cm_cv_results_tr = pd.DataFrame(data = [cm_cv_results['train_tn'], cm_cv_results['train_fp'], cm_cv_results['train_fn'], cm_cv_results['train_tp']]).T\n",
    "    cm_cv_results_tr.columns= ['train_tn', 'train_fp', 'train_fn', 'train_tp']\n",
    "    \n",
    "    print(\"Training Evaluation Metrics: \")\n",
    "    print(\"=============================\")\n",
    "    avg_cm_tr = np.array([(np.mean(cm_cv_results_tr['train_tn']), np.mean(cm_cv_results_tr['train_fp'])),\n",
    "                         (np.mean(cm_cv_results_tr['train_fn']), np.mean(cm_cv_results_tr['train_tp']))])\n",
    "    print(\"Average Confusion Matrix across training Folds: \\n{}\".format(avg_cm_tr))\n",
    "\n",
    "    sum_cm_tr = np.array([(np.sum(cm_cv_results_tr['train_tn']), np.sum(cm_cv_results_tr['train_fp'])),\n",
    "                          (np.sum(cm_cv_results_tr['train_fn']), np.sum(cm_cv_results_tr['train_tp']))])\n",
    "    print(\"Overall confusion matrix across training folds: \\n{}\".format(sum_cm_tr))\n",
    "\n",
    "    for col in reg_cv_results.keys():\n",
    "        if col.startswith('train'):\n",
    "            print('Average Training {}: {}'.format(col, round(np.mean(reg_cv_results[col]), 3)))\n",
    "\n",
    "    print('\\n')\n",
    "    \n",
    "    cm_cv_results_te = pd.DataFrame(data = [cm_cv_results['train_tn'], cm_cv_results['train_fp'], cm_cv_results['train_fn'], cm_cv_results['train_tp']]).T\n",
    "    cm_cv_results_te.columns= ['test_tn', 'test_fp', 'test_fn', 'test_tp']\n",
    "\n",
    "    print(\"Validation Evaluation Metrics: \")\n",
    "    print(\"=============================\")\n",
    "    avg_cm_te = np.array([(np.mean(cm_cv_results_te['test_tn']), np.mean(cm_cv_results_te['test_fp'])),\n",
    "                         (np.mean(cm_cv_results_te['test_fn']), np.mean(cm_cv_results_te['test_tp']))])\n",
    "    print(\"Average Confusion Matrix across validation Folds: \\n{}\".format(avg_cm_te))\n",
    "\n",
    "    sum_cm_te = np.array([(np.sum(cm_cv_results_te['test_tn']), np.sum(cm_cv_results_te['test_fp'])),\n",
    "                          (np.sum(cm_cv_results_te['test_fn']), np.sum(cm_cv_results_te['test_tp']))])\n",
    "    print(\"Overall confusion matrix across validation folds: \\n{}\".format(sum_cm_te))\n",
    "\n",
    "    for col in reg_cv_results.keys():\n",
    "        if col.startswith('test'):\n",
    "            print('Average Validation {}: {}'.format(col, round(np.mean(reg_cv_results[col]), 3)))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to use cross-validation to get roc curve across folds\n",
    "# NOTE: I completely stole this from an answer to the question at https://stackoverflow.com/questions/29656550/how-to-plot-pr-curve-over-10-folds-of-cross-validation-in-scikit-learn\n",
    "from numpy import interp\n",
    "from sklearn.metrics import accuracy_score, auc, average_precision_score, confusion_matrix, roc_curve, precision_recall_curve\n",
    "def draw_cv_roc_curve(classifier, cv, X, y, title='ROC Curve'):\n",
    "    \"\"\"\n",
    "    Draw a Cross Validated ROC Curve.\n",
    "    Args:\n",
    "        classifier: Classifier Object\n",
    "        cv: StratifiedKFold Object: (https://stats.stackexchange.com/questions/49540/understanding-stratified-cross-validation)\n",
    "        X: Feature Pandas DataFrame\n",
    "        y: Response Pandas Series\n",
    "    Example largely taken from http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py\n",
    "    \"\"\"\n",
    "    # Creating ROC Curve with Cross Validation\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    i = 0\n",
    "    for train, test in cv.split(X, y):\n",
    "        probas_ = classifier.fit(X.iloc[train], y.iloc[train]).predict_proba(X.iloc[test])\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = roc_curve(y.iloc[test], probas_[:, 1])\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        \n",
    "        tprs[-1][0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "                 label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "        i += 1\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "             label='Luck', alpha=.8)\n",
    "    \n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "             label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "             lw=2, alpha=.8)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                     label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also stolen - Precision-Recall curve\n",
    "def draw_cv_pr_curve(classifier, cv, X, y, title='PR Curve'):\n",
    "    \"\"\"\n",
    "    Draw a Cross Validated PR Curve.\n",
    "    Keyword Args:\n",
    "        classifier: Classifier Object\n",
    "        cv: StratifiedKFold Object: (https://stats.stackexchange.com/questions/49540/understanding-stratified-cross-validation)\n",
    "        X: Feature Pandas DataFrame\n",
    "        y: Response Pandas Series\n",
    "        \n",
    "    Largely taken from: https://stackoverflow.com/questions/29656550/how-to-plot-pr-curve-over-10-folds-of-cross-validation-in-scikit-learn\n",
    "    \"\"\"\n",
    "    y_real = []\n",
    "    y_proba = []\n",
    "\n",
    "    i = 0\n",
    "    for train, test in cv.split(X, y):\n",
    "        probas_ = classifier.fit(X.iloc[train], y.iloc[train]).predict_proba(X.iloc[test])\n",
    "        # Compute ROC curve and area the curve\n",
    "        precision, recall, _ = precision_recall_curve(y.iloc[test], probas_[:, 1])\n",
    "        \n",
    "        # Plotting each individual PR Curve\n",
    "        plt.plot(recall, precision, lw=1, alpha=0.3,\n",
    "                 label='PR fold %d (AUC = %0.2f)' % (i, average_precision_score(y.iloc[test], probas_[:, 1])))\n",
    "        \n",
    "        y_real.append(y.iloc[test])\n",
    "        y_proba.append(probas_[:, 1])\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    y_real = np.concatenate(y_real)\n",
    "    y_proba = np.concatenate(y_proba)\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_real, y_proba)\n",
    "\n",
    "    plt.plot(recall, precision, color='b',\n",
    "             label=r'Precision-Recall (AUC = %0.2f)' % (average_precision_score(y_real, y_proba)),\n",
    "             lw=2, alpha=.8)\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Let's start with detecting evil robots through logistic regression. Before going ahead though, let me give a little overview on it in lecture. After that's done, implement a logistic regression model for detection below using k-fold cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "num_folds = # Put your choice for k here\n",
    "\n",
    "# We're going to do something a little weird by including a confusion matrix in our scores to be used in evaluation below\n",
    "# see https://scikit-learn.org/stable/modules/model_evaluation.html#using-multiple-metric-evaluation\n",
    "\n",
    "lr_pipe = imbPipe(\n",
    "    [\n",
    "        ('scaler', StandardScaler()), # PUT SCALING STEP IN HERE WITH ANY OPTIONS YOU THINK APPROPRIATE),\n",
    "        ('knn_imp', KNNImputer(n_neighbors= 5)), #PUT KNN IMPUTER HERE WITH ANY OPTIONS YOU THINK APPROPRIATE)\n",
    "        ('smote', SMOTE(random_state=1234)), # I went ahead and did the SMOTE for you\n",
    "        ('lr', LogisticRegression(random_state=1234)) # Anything else we should specify here?\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "lr_cv = cross_validate(# What goes here?,\n",
    "                        #X = ?, y = ?, \n",
    "                        scoring= scoring_dict, # Let's get a bunch of metrics \n",
    "                        cv = kf , return_estimator=True, return_train_score=True,\n",
    "                        return_estimator =True) # Let's return train scores to detect overfitting\n",
    "\n",
    "# Couldn't get the confusion matrix scorer to work in conjunction with others, so let's keep it separate\n",
    "lr_cv_cm = cross_validate(# What goes here,\n",
    "                        #X = ?, y = ?, \n",
    "                        scoring= confusion_matrix_scorer, # Let's get a bunch of metrics \n",
    "                        cv = kf , return_train_score=True) # Let's return train scores to detect overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Let's do the same thing using the k-Nearest Neighbors algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = #? # Pick your preferred k\n",
    "\n",
    "knn_pipe = imbPipe(\n",
    "    [\n",
    "        ('scaler', StandardScaler()), # PUT SCALING STEP IN HERE WITH ANY OPTIONS YOU THINK APPROPRIATE),\n",
    "        ('knn_imp', KNNImputer(n_neighbors= 5)), #PUT KNN IMPUTER HERE WITH ANY OPTIONS YOU THINK APPROPRIATE)\n",
    "        ('smote', SMOTE(random_state=1234)), # I went ahead and did the SMOTE for you\n",
    "        ('knn', KNeighborsClassifier(n_neighbors = k, metric = 'euclidean')) # Anything else we should specify here?\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "knn_cv = cross_validate(# What goes here?,\n",
    "                        #X = ?, y = ?, \n",
    "                        scoring= scoring_dict, # Let's get a bunch of metrics \n",
    "                        cv = kf , return_estimator=True, return_train_score=True,\n",
    "                        return_estimator =True) # Let's return train scores to detect overfitting\n",
    "\n",
    "# Couldn't get the confusion matrix scorer to work in conjunction with others, so let's keep it separate\n",
    "knn_cv_cm = cross_validate(# What goes here?,\n",
    "                        #X = ?, y = ?, \n",
    "                        scoring= confusion_matrix_scorer, # Let's get a bunch of metrics \n",
    "                        cv = kf , return_estimator=True, return_train_score=True,\n",
    "                        return_estimator =True) # Let's return train scores to detect overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Will Naïve Bayes classification work. Let's figure out what we're doing first? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# TODO figure out what to do with age variable - not really Gaussian now, is it?\n",
    "\n",
    "gnb_pipe = imbPipe(\n",
    "    [\n",
    "        ('scaler', StandardScaler()), # PUT SCALING STEP IN HERE WITH ANY OPTIONS YOU THINK APPROPRIATE),\n",
    "        ('knn_imp', KNNImputer(n_neighbors= 5)), #PUT KNN IMPUTER HERE WITH ANY OPTIONS YOU THINK APPROPRIATE)\n",
    "        ('smote', SMOTE(random_state=1234)), # I went ahead and did the SMOTE for you\n",
    "        ('gnb', GaussianNB()) # Anything else we should specify here?\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "gnb_cv = cross_validate(# What goes here?,\n",
    "                        #X = ?, y = ?, \n",
    "                        scoring= scoring_dict, # Let's get a bunch of metrics \n",
    "                        cv = kf , return_estimator=True, return_train_score=True,\n",
    "                        return_estimator =True) # Let's return train scores to detect overfitting\n",
    "\n",
    "\n",
    "# Couldn't get the confusion matrix scorer to work in conjunction with others, so let's keep it separate\n",
    "gnb_cv_cm = cross_validate(# What goes here?,\n",
    "                        #X = ?, y = ?, \n",
    "                        scoring= confusion_matrix_scorer, # Let's get a bunch of metrics \n",
    "                        cv = kf , return_estimator=True, return_train_score=True,\n",
    "                        return_estimator =True) # Let's return train scores to detect overfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. For our last classification algorithm of the day, let's take a look at support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "svc_pipe = imbPipe(\n",
    "    [\n",
    "        ('scaler', StandardScaler()), # PUT SCALING STEP IN HERE WITH ANY OPTIONS YOU THINK APPROPRIATE),\n",
    "        ('knn_imp', KNNImputer(n_neighbors= 5)), #PUT KNN IMPUTER HERE WITH ANY OPTIONS YOU THINK APPROPRIATE)\n",
    "        ('smote', SMOTE(random_state=1234)), # I went ahead and did the SMOTE for you\n",
    "        ('svc', SVC()) # Anything else we should specify here?\n",
    "    ]\n",
    ")\n",
    "\n",
    "num_folds = 5# Put your choice for k here\n",
    "\n",
    "\n",
    "svc_cv = cross_validate(# What goes here?,\n",
    "                        #X = ?, y = ?, \n",
    "                        scoring= score_dict, # Let's get a bunch of metrics \n",
    "                        cv = kf , return_estimator=True, return_train_score=True,\n",
    "                        return_estimator =True) # Let's return train scores to detect overfitting\n",
    "\n",
    "# Couldn't get the confusion matrix scorer to work in conjunction with others, so let's keep it separate\n",
    "svc_cv_cm = cross_validate(# What goes here?,\n",
    "                        #X = ?, y = ?, \n",
    "                        scoring= confusion_matrix_scorer, # Let's get a bunch of metrics \n",
    "                        cv = kf , return_estimator=True, return_train_score=True,\n",
    "                        return_estimator =True) # Let's return train scores to detect overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on Training/Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Now, just run the next two cells to see some evaluation metrics. Note that the \"test\" or \"validation\" data is measured only on the holdout folds of the training data - we still haven't touched the true test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_dict = {'Logistic Regression': [lr_cv, lr_cv_cm, lr_pipe],\n",
    "                     'K-Nearest-Neighbors': [knn_cv, knn_cv_cm, knn_pipe],\n",
    "                     'Gaussian Naive Bayes': [gnb_cv, gnb_cv_cm, gnb_pipe],\n",
    "                     'Support Vector Machine': [svc_cv, svc_cv_cm, svc_pipe]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in full_results_dict.keys():\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print('+++ EVALUATION METRICS FOR {0} +++'.format(model))\n",
    "    print('+++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    get_eval_metrics_report(full_results_dict[model][0], full_results_dict[model][1])\n",
    "    draw_cv_roc_curve(full_results_dict[model][2], X = robots_train_nans, y = evil_train['evil'], title = '{0} ROC Curve on Train and Validation Folds'.format(model))\n",
    "    draw_cv_pr_curve(full_results_dict[model][2], X = robots_train_nans, y = evil_train['evil'], title = '{0} PR Curve on Train and Validation Folds'.format(model))\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print('\\n\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAUSE: \n",
    "In a real life situation what should have we done and what should we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together and evaluating on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Let's put a bow on it, look at our metrics, get some visuals, evaluate on test data, then call it a day for this exercise, ok? First, let's get all our metrics together and make some predictions, and then use those to get a ROC curve. Normally, at this point we'd start evaluating and choosing hyperparameters for our model, but that's in another couple weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the models and evaluatate on Test - get ROC curve:\n",
    "\n",
    "pipeline_dict = {'Logistic Regression': lr_pipe,\n",
    "                'K-Nearest-Neighbors': knn_pipe,\n",
    "                'Gaussian Naive Bayes': gnb_pipe,\n",
    "                'Support Vector Machine': svc_pipe\n",
    "}\n",
    "\n",
    "#Finally let's see how we do on the test set:\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "for model in pipeline_dict.keys():\n",
    "    y_pred = pipeline_dict[model].predict_proba(X = robots_test)\n",
    "    fpr, tpr, thresholds = roc_curve(evil_test['evil'].to_numpy(), y_pred[:,1])\n",
    "    auc_val = auc(fpr, tpr)\n",
    "\n",
    "    plt.plot(fpr, tpr, label='{0} (AUC = {1})'.format(model, round(auc_val,3)))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('ROC on Test Set')\n",
    "plt.xlabel('False Postive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And lastly get some classification reports\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for model in full_results_dict.keys():\n",
    "    print(model)\n",
    "    y_pred = full_results_dict[model][2].predict(X = robots_test)\n",
    "    print(classification_report(evil_test['evil'].to_numpy(), y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
